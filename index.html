<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Efficient Common Sense in LLMs via Knowledge Graph Compression</title>
  <link rel="stylesheet" href="style.css" />
  <meta name="description" content = "UCSD HDSI Data Science DSC180B Capstone Project">
  <meta name="google-site-verification" content="AqpJWZR7mF0TUiBkwljBCtdEj6GHXUa6W445wavO7Ps" />
  
  <script src = 'script.js'></script>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@100;300;400;500;700&display=swap" rel="stylesheet">
</head>
<body>

    <body>
        <div class="container">
          <div class="sidebar">
            <a href="#problem">Problem</a>
            <a href="#data">Data</a>
            <a href="#project">Project</a>
            <a href="#team">Team</a>
          </div>
          <div class="main">
            <div class="header">
                <div class="header-content">
              <h1>Efficient Common Sense in LLMs via Knowledge Graph Compression</h1>

              <h3>Quentin Callahan, Esther Cho, Penny King</h3>
              
              <div class="buttons">
                <a href="https://github.com/PenelopeKing/Efficient-Common-Sense-in-LLMs-via-Knowledge-Graph-Compression" target="_blank" class="button">GitHub Repo</a>
                <a href="https://github.com/estherch0/artifact-directory/blob/main/report.pdf" target="_blank" class="button">Paper</a>
                <a href="https://drive.google.com/file/d/1TggEVpC3iGciQVlMHNFu9MKXu7TgZp3U/view" target="_blank" class="button">Poster</a>
              </div>

              

            </div>
            </div>
            <div class="content">
              <div class = "content2" >
              <section class="fade-section" id="problem">
                <h2>THE PROBLEM</h2>

                <div class="image-container">
                  <img class="probimg" src="assets/figures/problem1.png" alt="problem statement 1 representation">
                  <img class="probimg" src="assets/figures/problem2.png" alt="problem statement 2 representation">
                  <img class="probimg" src="assets/figures/problem3.png" alt="problem statement 3 representation">
              </div>

                <p>
                  Our project tackles a fundamental challenge in AI: teaching machines common sense. 
                  Specifically we are exploring how Large Language Models (LLM) deal with common sense and intuition.
                  Commonsense typically comes easily to humans. 
                  While humans naturally understand that "dropping a glass cup will break it," 
                  machines struggle with this type of intuitive knowledge. 
                </p>
                <p>
                  To help with this, data scientists use <b>knowledge graphs</b>, which are structured graph networks of information that connect concepts to each other along with their relationships. 
                  For example, how a “dog” (concept 1) “is a” (relationship) "mammal" (concept 2). 
                </p>

                <img class = 'img'  src="assets/figures/example_kg.png" alt="a small knowledge graph" >
          

                <p>But there's one “big” problem: these graphs are REALLY REALLY BIG.</p>

                
                <p>
                  Feeding all this information into LLMs can overwhelm them, leading to confusion and poor performance. 
                  This mirrors much like how humans also get overwhelm with a huge overflow of information.
 
                  <em>So, how can we compress these knowledge graphs to be smaller and still effective at teaching common sense to LLMs?
                  </em>
                  </p>

                  <h3 class="subtitle">Our Solution: Transformers</h3>
                   
                  <p>Transformers (specifically Graph Transformers) may be the solution for compressing knowledge graphs.
                    
                  Graph Transformers are a type of deep learning model that extends the transformer architecture to work with graph structured data. 
                  It uses the power of self-attention mechanisms and graph learning to complex complex relationships between nodes in a graph.  
                  Although they require hefty computational power, transformers are especially great at handling long-range and complex relationships especially on large graphs.  

                  </p>

                  <p>
                    Being able to feed commonsense into LLMs in a more effective manner through compressed knowledge graphs can lead to better virtual assistants, improved AI decision making, and more natural interactions with technology.
                  </p>
                
            
              </section>
              <section class="fade-section" id="data">
                <h2>THE DATA</h2>
                <p>The graph data we are specifically working with are Common Sense Knowledge Graphs (CSKGs). 
                  CSKGs are a specialized type of knowledge graph designed to encode general world knowledge. 
                  They play a crucial role in various applications, including reasoning, decision-making, and natural language understanding. 
                  They can assist LLMs in generating commonsense explanations beyond what is explicitly mentioned in context. 
                  And compressing CSKGs can ensure that a LLM is fed concise knowledge without redundant or irrelevant information. 
                  </p>
                <p>
                  The data we used consisted of two CSKGs: ComVE and α-NLG. 
                </p>
                
                <details>
                  <summary><b>ComVE</b></summary>
                  <p>In ComVE the goal is to generate explanations on why a nonsensical sentence does not make sense. 
                    Each sample comes with three reference output sentences, which are human-written explanations. 
                    The dataset has a training size of 20k, and a test and validation size of 1000.</p>
                    <p>Example: "We use books to tell the time" would result in the LLM generating a response like "books are used to read, not to tell the time."</p>
                </details>
                
                <details>
                  <summary><b>α-NLG</b></summary>
                  <p>For α-NLG, the task is to generate a plausible explanation for what might have happened between a past and future observation, 
                    which is also known as abductive reasoning. Each sample in the dataset includes up to 5 reference outputs. 
                    This dataset has 50k training points, over 1,500 validation points, and over 3,500 test data points.</p>
                    <p>Example: The LLM is fed a past action: "Clouds appeared in the sky." And a future action: "The sidewalk was wet." These input prompts would then result in a response like: "It rained."</p>
                  </details>

              
              </section>


              <section class="fade-section" id="project">
                <h2>THE PROJECT</h2>
                <h3 class="subtitle">Methods</h3>
                <p>
                  We aimed to help AI understand common sense better by making large collections of knowledge easier to handle. Below is a general outline of how our methodology works:
                </p>
                
                <div style="text-align: center;">
                  <img src="assets/figures/overview_transparent.png" alt="Project pipeline" style="width:100%;max-width:800px;">
                </div>
                <p><b>1. Finding Important Information</b></p>

                <p>We started by picking out the key ideas from a sentence. For example, from "A person cannot walk across water because water is not solid," the important words are "person," "walk," "water," and "solid." We then looked for related ideas connected to these words in the knowledge graph.
                </p>

                <p><b>2. Compression: Choosing the Best Connections</b></p>

                <p>We used a special model called a graph transformer to decide which connections were the most important. Think of it like a highlighter that picks out the most relevant ideas while ignoring less useful details. This helps the computer focus on what really matters for understanding the meaning.
                  </p>

                  <details>
                    <summary><b>About Transformers</b></summary>
                    <p>
                      Traditional NLP transformers were built to work on fully connected 
                      and often sequential sequences. 
                      However, graph data has a topology that is often large, 
                      complex and does not guarantee full connectivity. 
                      NLP transformer models largely use positional 
                      encodings for words to ensure unique representation and 
                      preserve distance information about each word. 
                      This has been adapted in graph transformers by 
                      fusing node positional features using Laplacian eigenvectors for graph data. 
                      This technique is an effective way to encode node positional information in 
                      complex graph data.</p>
                    <p> 
                      Transformers excel on large graph data due to their ability to capture 
                      long-range dependencies and complex relationships between nodes, 
                      thanks to the self-attention mechanism that considers all node interactions simultaneously. 
                      This global receptive field allows them to effectively model intricate structures 
                      and dependencies that traditional Graph Neural Networks (GNNs) struggle with due to 
                      limited neighborhood aggregation. 
                      Additionally, transformers are highly parallelizable and scalable, 
                      making them well-suited for processing large graphs efficiently, 
                      especially when combined with sparse attention techniques to handle graph sparsity.
                    </p>
                  </details>

                <p><b>3. Turning Connections into Text</b></p>
                <p>Instead of just feeding the computer a list of words, we turned the connections into short text explanations. This gave the computer more context and helped it understand the relationships better.

                </p>

                <p><b>4. Training the LLM</b></p>
                <p>We trained our a BART-Based LLM using two tasks:
                </p>

                <p style="margin-left:50px;"><b style="color:var(--chestnut-color);">Explaining Nonsense:</b> We gave the model strange sentences and asked it to explain why they didn't make sense.                </p>
                <p style="margin-left:50px;"><b style="color:var(--chestnut-color);">Guessing What Happened:</b> We showed it two events and asked it to guess what happened in between (abductive reasoning).</p>

                <p><b>5. Measuring Success </b></p>
                <p>We checked how well our model did by measuring three things:
                </p>

                
                <details>
                  <summary><b style="color:var(--chestnut-color);">Variety across multiple LLM generated sentences.</b></summary>
                    <p>Did the LLM come up with different explanations each time?</p>
                    <p>This is also known as pairwise diversity. The metric used to measure this is known as Self-BLEU, which evaluates how a sentence is similar to other generated sentences (from the same input prompt) based on n-gram overlap.</p>
                  </details>
                  <details>
                  <summary> <b style="color:var(--chestnut-color);">Variety within a singular LLM generated sentence.</b></summary>
                  <p>Is there a variety of words within the generated sentences?</p>
                  <p>This is also known as corpus diversity, and is measured with Entropy-k and Distinct-k. Entropy-k evaluates evenness of empirical n-gram distribution within generated sentence
                    Distinct-k looks at the number of unique k-grams in the generated sentences and divides it by total number of generated tokens; this prevents preference towards longer sentences in the LLM's output.
                    </p>
                </details>
                <details>
                  <summary><b style="color:var(--chestnut-color);">Quality of LLM generated vs reference sentences.</b></summary>
                  <p>Were the explanations accurate and reasonable? </p>
                  <p>This was measured by BLEU and ROUGE, which looks specifically at <a href="https://en.wikipedia.org/wiki/Precision_and_recall" target="_blank">precision and recall</a> in the human answers with the LLM generated answers. BLEU is the precision of n-grams in the generated output against the reference text. And ROUGE is the recall of n-grams in generated output against the reference text.
                    </p>
              </details>

              <p>
                We trained and tested 4 types of BART-Based LLM architectures involving a mixture of compression and no compression to see how a Transformer based compression architecture would perform in comparison.
                These 4 models were: a LLM with no knowledge graph, a LLM with an uncompressed knowledge graph, a LLM with a RGCN-compressed knowledge graph, and our LLM with a Transformer-compressed knowledge graph.
                Our RGCN compression model is based off the research performed by <a href="https://aclanthology.org/2023.emnlp-main.37.pdf" target ="_blank">Hwang et al (2023)</a> . Further details about the Transformer model we created are discussed further in <a href="https://github.com/estherch0/artifact-directory/blob/main/report.pdf" target = "_blank">our report</a>.
              </p>


                <h3 class="subtitle">Results</h3>
                <p>Using the table below, you can see how the 4 tested models performed with various datasets. An arrow going up indicates if having a higher number between 0 and 100 indicates better performance. And an arrow pointing down indicates the opposite. </p>
                <div style="text-align: center;">
                  <img src="assets/results.png" alt="Results visualization" style="width:100%;max-width:600px;">
                </div>
                
                <p>The results of our experimentation resulted in a Transformer compression model that could relatively better than a simpler RGCN compression model. 
                  However (looking at models with the same training compute time) the performance of the Transformer model is mostly observed in some diversity metrics and in our recall metric (ROUGE-L).
                </p>
                <p>Noteably, the Transformer model exhibits decreased accuracy in the ComVE dataset compared to the αNLG dataset likely due the innate structure of a Transformed based model. 
                  Transformers specialize in capturing longer range relationships between different relationships between concepts, which may not be necessary for a simpler common sense reasoning task.
                  However for an abductive reasoning task which requires the LLM to infer what happens between two events, such relationships may be more beneficial.
                </p>

                
                <p>
                  We also included a sneak peek into what the Transformer compression model may behave like with more time dedicated for training and tuning. 
                  These better results in "+20k steps" indicate the potential for better performance in our Transformer model with increased training time.
                  There is also some ambiguity of results between the baseline models (no KG and uncompressed KG) with the compressed KG models which should be addressed in further research. 
                  Unfortunately, due to time constraints further comparison and research will have to be delegated another time.
                </p>

                  <h3 class="subtitle">Conclusion</h3>

                  <p>
                    The varying performance of our model at this stage of our research right now suggests that performance in Transformer compression may be tied to task and limitations within
                    the dataset (size, existence of long range dependencies, etc). 
                    Transformers often perform at their peak when a dataset is larger and has long range dependencies (interactions) available for it to capture and then learn off of.
                  </p>
                  <p>However, our project shows that there may be benefits for employing a Transformer based architecutre for the compression of knowledge graphs. 
                    Being able to tailor compress a knowledge graph will allow a LLM to perform at a higher level in terms of interacting with humans and understanding user intent.</p>
                  <p>Ultimately the goal of effective compression of common sense knowledge graphs is not just about teaching AI to “know” more, but how to “understand” more intelligently and intuitively. 
                  </p>
                    <p>
                    Effectively incorporating common sense knowledge in LLMs can lead to smarter virtual assitants that can better understand user intent. 
                    As well as improved AI decision making for more accurate and logical responses. It can also allow machines to interact more naturally for human thinking and logic, specifically when it comes to the nuances of human intuition.
                  </p>

              </section>
              </div>

              <section id="team" class="team">
                <h2>THE TEAM</h2>
                <div class="team-members">
                  <div class="team-member">
                    <div class="member-image">
                      <a href="https://github.com/Qcallahan" target="_blank">
                      <img src="assets/pics/quentin.jpeg" alt="Quentin Callahan">
                    </a>
                    </div>
                    <h3>Quentin Callahan</h3>
                    <p>Data Science Major</p>
                    <p>Quentin is passionate about machine learning, spicy food, reading, and making code run fast. 
                      He is interested in how graph theory can be used to make existing algorithms more efficient.</p>

                  </div>
                  <div class="team-member">
                    <div class="member-image">
                      <a href="https://github.com/estherch0" target="_blank">
                      <img src="assets/pics/esther.jpeg" alt="Esther Cho">
                    </a>
                    </div>
                    <h3>Esther Cho</h3>
                    <p>Data Science Major & Math Minor</p>
                    <p>Esther enjoys exploring the practical applications of machine learning, especially the mathematical aspects like graph theory. 
                      In her free time, she enjoys playing volleyball and flag football, working on puzzles, and hiking new trails.</p>

                  </div>
                  <div class="team-member">
                    <div class="member-image">
                      <a href="https://github.com/PenelopeKing" target="_blank">
                      <img src="assets/pics/penny.JPG" alt="Penny King">
                    </a>
                    </div>
                    <h3>Penny King</h3>
                    <p>Data Science Major</p>
                    <p>Penny is curious about how graph-based algorithms can enhance data science applications. 
                      She also enjoys reading and exploring tea cultures from around the world. 
                      Learn more about her <a href="https://penelopeking.github.io/" target="_blank">here</a>.</p>
                  </div>
                </div>

                <div class="special-thanks">
                  
                  <p style="margin-bottom:-20px;">We would like to give a special thanks to our mentors at HDSI:</p>
                  <p><strong>Yusu Wang & Gal Mishne</strong></p>
                </div>
              </section>
            </div>
          </div>
        </div>
      </body>
      

      
</body>
</html>


  