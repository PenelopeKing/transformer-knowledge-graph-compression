<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Efficient Common Sense in LLMs via Knowledge Graph Compression</title>
  <link rel="stylesheet" href="style.css" />
  <meta name="description" content = "UCSD HDSI Data Science DSC180B Capstone Project">
  <meta name="google-site-verification" content="AqpJWZR7mF0TUiBkwljBCtdEj6GHXUa6W445wavO7Ps" />
  
  <script src = 'script.js'></script>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@100;300;400;500;700&display=swap" rel="stylesheet">
</head>
<body>

    <body>
        <div class="container">
          <div class="sidebar">
            <a href="#problem">Problem</a>
            <a href="#data">Data</a>
            <a href="#project">Project</a>
            <a href="#team">Team</a>
          </div>
          <div class="main">
            <div class="header">
                <div class="header-content">
              <h1>Efficient Common Sense in LLMs via Knowledge Graph Compression</h1>

              <h3>Quentin Callahan, Esther Cho, Penny King</h3>
              
              <div class="buttons">
                <a href="https://github.com/PenelopeKing/Efficient-Common-Sense-in-LLMs-via-Knowledge-Graph-Compression" target="_blank" class="button">GitHub Repo</a>
                <a href="https://github.com/estherch0/artifact-directory/blob/main/report.pdf" target="_blank" class="button">Paper</a>
                <a href="https://drive.google.com/file/d/1TggEVpC3iGciQVlMHNFu9MKXu7TgZp3U/view" target="_blank" class="button">Poster</a>
              </div>

              

            </div>
            </div>
            <div class="content">
              <div class = "content2" >
              <section class="fade-section" id="problem">
                <h2>THE PROBLEM</h2>

                <div class="image-container">
                  <img class="probimg" src="assets/figures/problem1.png" alt="problem statement 1 representation">
                  <img class="probimg" src="assets/figures/problem2.png" alt="problem statement 2 representation">
                  <img class="probimg" src="assets/figures/problem3.png" alt="problem statement 3 representation">
              </div>

                <p>
                  Our project tackles a fundamental challenge in AI: teaching machines common sense. 
                  Specifically we are exploring how Large Language Models (LLM) deal with common sense and intuition.
                  Commonsense typically comes easily to humans. 
                  While humans naturally understand that "dropping a glass cup will break it," 
                  machines struggle with this type of intuitive knowledge. 
                </p>
                <p>
                  To help with this, data scientists use <b>knowledge graphs</b>, which are structured graph networks of information that connect concepts to each other along with their relationships. 
                  For example, how a “dog” (concept 1) “is a” (relationship) "mammal" (concept 2). 
                </p>

                <img class = 'img'  src="assets/figures/example_kg.png" alt="a small knowledge graph" >
          

                <p>But there's one “big” problem: these graphs are REALLY REALLY BIG.</p>

                
                <p>
                  Feeding all this information into LLMs can overwhelm them, leading to confusion and poor performance. 
                  This mirrors much like how humans also get overwhelm with a huge overflow of information.
 
                  <em>So, how can we compress these knowledge graphs to be smaller and still effective at teaching common sense to LLMs?
                  </em>
                  </p>

                  <h3 class="subtitle">Our Solution: Transformers</h3>
                   
                  <p>Transformers (specifically Graph Transformers) may be the solution for compressing knowledge graphs.
                    
                  Graph Transformers are a type of deep learning model that extends the transformer architecture to work with graph structured data. 
                  It uses the power of self-attention mechanisms and graph learning to complex complex relationships between nodes in a graph.  
                  Although they require hefty computational power, transformers are especially great at handling long-range and complex relationships especially on large graphs.  

                  </p>

                  <p>
                    Being able to feed commonsense into LLMs in a more effective manner through compressed knowledge graphs can lead to better virtual assistants, improved AI decision making, and more natural interactions with technology.
                  </p>
                
            
              </section>
              <section class="fade-section" id="data">
                <h2>THE DATA</h2>
                <p>The graph data we are specifically working with are Common Sense Knowledge Graphs (CSKGs). 
                  CSKGs are a specialized type of knowledge graph designed to encode general world knowledge. 
                  They play a crucial role in various applications, including reasoning, decision-making, and natural language understanding. 
                  They can assist LLMs in generating commonsense explanations beyond what is explicitly mentioned in context. 
                  And compressing CSKGs can ensure that a LLM is fed concise knowledge without redundant or irrelevant information. 
                  </p>
                <p>
                  The data we used consisted of two CSKGs: ComVE and α-NLG. 
                </p>
                
                <details>
                  <summary><b>ComVE</b></summary>
                  <p>In ComVE the goal is to generate explanations on why a nonsensical sentence does not make sense. 
                    Each sample comes with three reference output sentences, which are human-written explanations. 
                    The dataset has a training size of 10k, and a test and validation size of 1000.</p>
                </details>
                
                <details>
                  <summary><b>α-NLG</b></summary>
                  <p>For α-NLG, the task is to generate a plausible explanation for what might have happened between a past and future observation, 
                    which is also known as abductive reasoning. Each sample in the dataset includes up to 5 reference outputs. 
                    This dataset has 50k training points, over 1,500 validation points, and over 3,500 test data points.</p>
                </details>

              
              </section>


              <section class="fade-section" id="project">
                <h2>THE PROJECT</h2>
                <h3 class="subtitle">Methods</h3>
                <p>
                  We aimed to help AI understand common sense better by making large collections of knowledge easier to handle. Below is a general outline of how our methodology works:
                </p>
                
                <div style="text-align: center;">
                  <img src="assets/figures/overview_transparent.png" alt="Project pipeline" style="width:100%;max-width:800px;">
                </div>
                <p><b>1. Finding Important Information</b></p>

                <p>We started by picking out the key ideas from a sentence. For example, from "A person cannot walk across water because water is not solid," the important words are "person," "walk," "water," and "solid." We then looked for related ideas connected to these words in the knowledge graph.
                </p>

                <p><b>2. Compression: Choosing the Best Connections</b></p>

                <p>We used a special model called a graph transformer to decide which connections were the most important. Think of it like a highlighter that picks out the most relevant ideas while ignoring less useful details. This helps the computer focus on what really matters for understanding the meaning.
                  </p>

                  <details>
                    <summary><b>About Transformers</b></summary>
                    <p>
                      Traditional NLP transformers were built to work on fully connected 
                      and often sequential sequences. 
                      However, graph data has a topology that is often large, 
                      complex and does not guarantee full connectivity. 
                      NLP transformer models largely use positional 
                      encodings for words to ensure unique representation and 
                      preserve distance information about each word. 
                      This has been adapted in graph transformers by 
                      fusing node positional features using Laplacian eigenvectors for graph data. 
                      This technique is an effective way to encode node positional information in 
                      complex graph data.</p>
                    <p> 
                      Transformers excel on large graph data due to their ability to capture 
                      long-range dependencies and complex relationships between nodes, 
                      thanks to the self-attention mechanism that considers all node interactions simultaneously. 
                      This global receptive field allows them to effectively model intricate structures 
                      and dependencies that traditional Graph Neural Networks (GNNs) struggle with due to 
                      limited neighborhood aggregation. 
                      Additionally, transformers are highly parallelizable and scalable, 
                      making them well-suited for processing large graphs efficiently, 
                      especially when combined with sparse attention techniques to handle graph sparsity.
                    </p>
                  </details>

                <p><b>3. Turning Connections into Text</b></p>
                <p>Instead of just feeding the computer a list of words, we turned the connections into short text explanations. This gave the computer more context and helped it understand the relationships better.

                </p>

                <p><b>4. Training the LLM</b></p>
                <p>We trained our model using two tasks:
                </p>

                <p style="margin-left:50px;"><b style="color:var(--chestnut-color);">Explaining Nonsense:</b> We gave the model strange sentences and asked it to explain why they didn't make sense.                </p>
                <p style="margin-left:50px;"><b style="color:var(--chestnut-color);">Guessing What Happened:</b> We showed it two events and asked it to guess what happened in between (abductive reasoning).</p>

                <p><b>5. Measuring Success </b></p>
                <p>We checked how well our model did by measuring three things:
                </p>
                <p style="margin-left:50px;"><b style="color:var(--chestnut-color);">Variety across multiple LLM generated sentences:</b> Did the LLM come up with different explanations each time?</p>
                <p style="margin-left:50px;"><b style="color:var(--chestnut-color);">Variety within a singular LLM generated sentence:</b> Is there a variety of words within the generated sentences?</p>
                <p style="margin-left:50px;"><b style="color:var(--chestnut-color);">Quality of LLM generated vs reference sentences:</b>  Were the explanations accurate and reasonable?                </p>

                <h3 class="subtitle">Results</h3>
                <p></p>
                <img src="assets/results.png" alt="Results visualization" style="width:100%;max-width:400px;">
                  

                  <h3 class="subtitle">Conclusion</h3>
                  <p>Ultimately the goal of effective compression of common sense knowledge graphs is not just about teaching AI to “know” more, but how to “understand” more intelligently and intuitively. 
                  </p>
                    <p>
                    Effectively incorporating common sense knowledge in LLMs can lead to smarter virtual assitants that can better understand user intent. 
                    As well as improved AI decision making for more accurate and logical responses. It can also allow machines to interact more naturally for human thinking and logic, specifically when it comes to the nuances of human intuition.
                  </p>

              </section>
              </div>

              <section id="team" class="team">
                <h2>THE TEAM</h2>
                <div class="team-members">
                  <div class="team-member">
                    <div class="member-image">
                      <a href="https://github.com/Qcallahan" target="_blank">
                      <img src="assets/pics/quentin.jpeg" alt="Quentin Callahan">
                    </a>
                    </div>
                    <h3>Quentin Callahan</h3>
                    <p>Data Science Major</p>
                    <p>Quentin is passionate about machine learning, spicy food, reading, and making code run fast. 
                      He is interested in how graph theory can be used to make existing algorithms more efficient.</p>

                  </div>
                  <div class="team-member">
                    <div class="member-image">
                      <a href="https://github.com/estherch0" target="_blank">
                      <img src="assets/pics/esther.jpeg" alt="Esther Cho">
                    </a>
                    </div>
                    <h3>Esther Cho</h3>
                    <p>Data Science Major & Math Minor</p>
                    <p>Esther enjoys exploring the practical applications of machine learning, especially the mathematical aspects like graph theory. 
                      In her free time, she enjoys playing volleyball and flag football, working on puzzles, and hiking new trails.</p>

                  </div>
                  <div class="team-member">
                    <div class="member-image">
                      <a href="https://github.com/PenelopeKing" target="_blank">
                      <img src="assets/pics/penny.JPG" alt="Penny King">
                    </a>
                    </div>
                    <h3>Penny King</h3>
                    <p>Data Science Major</p>
                    <p>Penny enjoys exploring how graph-based algorithms can enhance areas like recommendation systems and transportation logistics. 
                      She also enjoys reading historical fiction novels and exploring tea cultures from around the world. 
                      Learn more about her <a href="https://penelopeking.github.io/" target="_blank">here</a>.</p>
                  </div>
                </div>

                <div class="special-thanks">
                  
                  <p style="margin-bottom:-20px;">We would like to give a special thanks to our mentors at HDSI:</p>
                  <p><strong>Yusu Wang & Gal Mishne</strong></p>
                </div>
              </section>
            </div>
          </div>
        </div>
      </body>
      

      
</body>
</html>


  